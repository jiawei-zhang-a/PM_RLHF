[2025-03-04 23:34:02,653] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/jiaweizhang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-04 23:34:06,928] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-03-04 23:34:06,929] [INFO] [runner.py:607:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --data_path Dahoas/rm-static --data_split 2,4,4 --actor_model_name_or_path --critic_model_name_or_path --num_padding_at_beginning 1 --per_device_generation_batch_size 8 --per_device_training_batch_size 8 --generation_batches 1 --ppo_epochs 1 --max_answer_seq_len 256 --max_prompt_seq_len 256 --actor_learning_rate 5e-4 --critic_learning_rate 5e-6 --num_train_epochs 1 --lr_scheduler_type cosine --gradient_accumulation_steps 16 --num_warmup_steps 100 --deepspeed --seed 1234 --actor_zero_stage 0 --critic_zero_stage 0 --actor_lora_dim 128 --actor_gradient_checkpointing --critic_gradient_checkpointing --actor_dropout 0.0 --enable_hybrid_engine --output_dir ./output
[2025-03-04 23:34:08,617] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/jiaweizhang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-04 23:34:11,964] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-03-04 23:34:11,964] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-03-04 23:34:11,964] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-03-04 23:34:11,964] [INFO] [launch.py:164:main] dist_world_size=1
[2025-03-04 23:34:11,964] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-03-04 23:34:11,965] [INFO] [launch.py:256:main] process 4178260 spawned with command: ['/opt/conda/bin/python', '-u', 'main.py', '--local_rank=0', '--data_path', 'Dahoas/rm-static', '--data_split', '2,4,4', '--actor_model_name_or_path', '--critic_model_name_or_path', '--num_padding_at_beginning', '1', '--per_device_generation_batch_size', '8', '--per_device_training_batch_size', '8', '--generation_batches', '1', '--ppo_epochs', '1', '--max_answer_seq_len', '256', '--max_prompt_seq_len', '256', '--actor_learning_rate', '5e-4', '--critic_learning_rate', '5e-6', '--num_train_epochs', '1', '--lr_scheduler_type', 'cosine', '--gradient_accumulation_steps', '16', '--num_warmup_steps', '100', '--deepspeed', '--seed', '1234', '--actor_zero_stage', '0', '--critic_zero_stage', '0', '--actor_lora_dim', '128', '--actor_gradient_checkpointing', '--critic_gradient_checkpointing', '--actor_dropout', '0.0', '--enable_hybrid_engine', '--output_dir', './output']
Traceback (most recent call last):
  File "/net/scratch/jiaweizhang/PM_RLHF/step3_ppo_finetuning/main.py", line 17, in <module>
    from torch.utils.tensorboard import SummaryWriter
  File "/home/jiaweizhang/.local/lib/python3.12/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
[2025-03-04 23:34:13,966] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 4178260
[2025-03-04 23:34:13,966] [ERROR] [launch.py:325:sigkill_handler] ['/opt/conda/bin/python', '-u', 'main.py', '--local_rank=0', '--data_path', 'Dahoas/rm-static', '--data_split', '2,4,4', '--actor_model_name_or_path', '--critic_model_name_or_path', '--num_padding_at_beginning', '1', '--per_device_generation_batch_size', '8', '--per_device_training_batch_size', '8', '--generation_batches', '1', '--ppo_epochs', '1', '--max_answer_seq_len', '256', '--max_prompt_seq_len', '256', '--actor_learning_rate', '5e-4', '--critic_learning_rate', '5e-6', '--num_train_epochs', '1', '--lr_scheduler_type', 'cosine', '--gradient_accumulation_steps', '16', '--num_warmup_steps', '100', '--deepspeed', '--seed', '1234', '--actor_zero_stage', '0', '--critic_zero_stage', '0', '--actor_lora_dim', '128', '--actor_gradient_checkpointing', '--critic_gradient_checkpointing', '--actor_dropout', '0.0', '--enable_hybrid_engine', '--output_dir', './output'] exits with return code = 1
